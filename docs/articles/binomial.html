<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Estimating Generalized Linear Models for Binary and Binomial Data with rstanarm • rstanarm</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">rstanarm</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../Articles/index.html">Articles</a>
</li>
<li>
  <a href="https://htmlpreview.github.io/?https://github.com/stan-dev/rstanarm/blob/master/dev-notes/rstanarm_dev_notes.html">Develop</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/stan-dev/rstanarm">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Estimating Generalized Linear Models for Binary and Binomial Data with rstanarm</h1>
                        <h4 class="author">Jonah Gabry and Ben Goodrich</h4>
            
            <h4 class="date">2017-06-15</h4>
          </div>

    
    
<div class="contents">
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{stan_glm: GLMs for Binary and Binomial Data}
-->
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>This vignette explains how to estimate generalized linear models (GLMs) for binary (Bernoulli) and Binomial response variables using the <code>stan_glm</code> function in the <strong>rstanarm</strong> package.</p>
<p>Steps 3 and 4 are covered in more depth by the vignette entitled <a href="rstanarm.html">“How to Use the <strong>rstanarm</strong> Package”</a>. This vignette focuses on Step 1 when the likelihood is the product of conditionally independent binomial distributions (possibly with only one trial per observation).</p>
</div>
<div id="likelihood" class="section level1">
<h1 class="hasAnchor">
<a href="#likelihood" class="anchor"></a>Likelihood</h1>
<p>For a binomial GLM the likelihood for one observation <span class="math inline">\(y\)</span> can be written as a conditionally binomial PMF <span class="math display">\[\binom{n}{y} \pi^{y} (1 - \pi)^{n - y},\]</span> where <span class="math inline">\(n\)</span> is the known number of trials, <span class="math inline">\(\pi = g^{-1}(\eta)\)</span> is the probability of success and <span class="math inline">\(\eta = \alpha + \mathbf{x}^\top \boldsymbol{\beta}\)</span> is a linear predictor. For a sample of size <span class="math inline">\(N\)</span>, the likelihood of the entire sample is the product of <span class="math inline">\(N\)</span> individual likelihood contributions.</p>
<p>Because <span class="math inline">\(\pi\)</span> is a probability, for a binomial model the <em>link</em> function <span class="math inline">\(g\)</span> maps between the unit interval (the support of <span class="math inline">\(\pi\)</span>) and the set of all real numbers <span class="math inline">\(\mathbb{R}\)</span>. When applied to a linear predictor <span class="math inline">\(\eta\)</span> with values in <span class="math inline">\(\mathbb{R}\)</span>, the inverse link function <span class="math inline">\(g^{-1}(\eta)\)</span> therefore returns a valid probability between 0 and 1.</p>
<p>The two most common link functions used for binomial GLMs are the <a href="https://en.wikipedia.org/wiki/Logit">logit</a> and <a href="https://en.wikipedia.org/wiki/Probit">probit</a> functions. With the logit (or log-odds) link function <span class="math inline">\(g(x) = \ln{\left(\frac{x}{1-x}\right)}\)</span>, the likelihood for a single observation becomes</p>
<p><span class="math display">\[\binom{n}{y}\left(\text{logit}^{-1}(\eta)\right)^y 
\left(1 - \text{logit}^{-1}(\eta)\right)^{n-y} = 
\binom{n}{y} \left(\frac{e^{\eta}}{1 + e^{\eta}}\right)^{y}
\left(\frac{1}{1 + e^{\eta}}\right)^{n - y}\]</span></p>
<p>and the probit link function <span class="math inline">\(g(x) = \Phi^{-1}(x)\)</span> yields the likelihood</p>
<p><span class="math display">\[\binom{n}{y} \left(\Phi(\eta)\right)^{y}
\left(1 - \Phi(\eta)\right)^{n - y},\]</span></p>
<p>where <span class="math inline">\(\Phi\)</span> is the CDF of the standard normal distribution. The differences between the logit and probit functions are minor and – if, as <strong>rstanarm</strong> does by default, the probit is scaled so its slope at the origin matches the logit’s – the two link functions should yield similar results. With <code>stan_glm</code>, binomial models with a logit link function can typically be fit slightly faster than the identical model with a probit link because of how the two models are implemented in Stan. Unless the user has a specific reason to prefer the probit link, we recommend the logit simply because it will be slightly faster and more numerically stable.</p>
<p>In theory, there are infinitely many possible link functions, although in practice only a few are typically used. Other common choices are the <code>cauchit</code> and <code>cloglog</code> functions, which can also be used with <code>stan_glm</code> (every link function compatible with<code>glm</code> will work with <code>stan_glm</code>).</p>
</div>
<div id="priors" class="section level1">
<h1 class="hasAnchor">
<a href="#priors" class="anchor"></a>Priors</h1>
</div>
<div id="posterior" class="section level1">
<h1 class="hasAnchor">
<a href="#posterior" class="anchor"></a>Posterior</h1>
<p>With independent prior distributions, the joint posterior distribution for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> is proportional to the product of the priors and the <span class="math inline">\(N\)</span> likelihood contributions:</p>
<p><span class="math display">\[f\left(\alpha,\boldsymbol{\beta} | \mathbf{y},\mathbf{X}\right) \propto
  f\left(\alpha\right) \times \prod_{k=1}^K f\left(\beta_k\right) \times
  \prod_{i=1}^N {
  g^{-1}\left(\eta_i\right)^{y_i} 
  \left(1 - g^{-1}\left(\eta_i\right)\right)^{n_i-y_i}}.\]</span></p>
<p>This is posterior distribution that <code>stan_glm</code> will draw from when using MCMC.</p>
</div>
<div id="logistic-regression-example" class="section level1">
<h1 class="hasAnchor">
<a href="#logistic-regression-example" class="anchor"></a>Logistic Regression Example</h1>
<p>When the logit link function is used the model is often referred to as a logistic regression model (the inverse logit function is the CDF of the standard logistic distribution). As an example, here we will show how to carry out a few parts of the analysis from Chapter 5.4 of <a href="http://www.stat.columbia.edu/~gelman/arm/">Gelman and Hill (2007)</a> using <code>stan_glm</code>.</p>
<p>Gelman and Hill describe a survey of 3200 residents in a small area of Bangladesh suffering from arsenic contamination of groundwater. Respondents with elevated arsenic levels in their wells had been encouraged to switch their water source to a safe public or private well in the nearby area and the survey was conducted several years later to learn which of the affected residents had switched wells. The goal of the analysis presented by Gelman and Hill is to learn about the factors associated with switching wells.</p>
<p>To start, we’ll use <code>dist</code> (the distance from the respondent’s house to the nearest well with safe drinking water) as the only predictor of <code>switch</code> (1 if switched, 0 if not). Then we’ll expand the model by adding the arsenic level of the water in the resident’s own well as a predictor and compare this larger model to the original.</p>
<p>After loading the <code>wells</code> data, we first rescale the <code>dist</code> variable (measured in meters) so that it is measured in units of 100 meters. If we leave <code>dist</code> in its original units then the corresponding regression coefficient will represent the effect of the marginal meter, which is too small to have a useful interpretation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rstanarm)
<span class="kw">data</span>(wells)
wells$dist100 &lt;-<span class="st"> </span>wells$dist /<span class="st"> </span><span class="dv">100</span></code></pre></div>
<p>Before estimating any models we can visualize the distribution of <code>dist100</code> in the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(wells, <span class="kw">aes</span>(<span class="dt">x =</span> dist100, <span class="dt">y =</span> ..density.., <span class="dt">fill =</span> switch ==<span class="st"> </span><span class="dv">1</span>)) +
<span class="st">  </span><span class="kw">geom_histogram</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">"gray30"</span>, <span class="st">"skyblue"</span>))</code></pre></div>
<p>In the plot above the blue bars correspond to the 1737 residents who said they switched wells and darker bars show the distribution of <code>dist100</code> for the 1283 residents who didn’t switch. As we would expect, for the residents who switched wells, the distribution of <code>dist100</code> is more concentrated at smaller distances.</p>
<p>A Bayesian version of Gelman and Hill’s initial logistic regression model can be estimated using the <code>stan_glm</code> function. Here we’ll use a Student t prior with 7 degrees of freedom and a scale of 2.5, which, as discussed above, is a reasonable default prior when coefficients should be close to zero but have some chance of being large.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t_prior &lt;-<span class="st"> </span><span class="kw"><a href="../reference/priors.html">student_t</a></span>(<span class="dt">df =</span> <span class="dv">7</span>, <span class="dt">location =</span> <span class="dv">0</span>, <span class="dt">scale =</span> <span class="fl">2.5</span>)
fit1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/stan_glm.html">stan_glm</a></span>(switch ~<span class="st"> </span>dist100, <span class="dt">data =</span> wells, 
                 <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">"logit"</span>), 
                 <span class="dt">prior =</span> t_prior, <span class="dt">prior_intercept =</span> t_prior,  
                 <span class="dt">chains =</span> CHAINS, <span class="dt">cores =</span> CORES, <span class="dt">seed =</span> SEED, <span class="dt">iter =</span> ITER)</code></pre></div>
<p>The <code>formula</code>, <code>data</code> and <code>family</code> arguments to <code>stan_glm</code> are specified in exactly the same way as for <code>glm</code>. We’ve also added the optional additional arguments <code>chains</code> (how many chains we want to execute), <code>cores</code> (how many cores we want the computer to utilize) and <code>seed</code> (for reproducibility). You can read about other possible arguments in the <code>stan_glm</code> documentation (<code>help(stan_glm, package = 'rstanarm')</code>).</p>
<p>To get a sense for the uncertainty in our estimates we can use the <code>posterior_interval</code> function to get Bayesian uncertainty intervals. The uncertainty intervals are computed by finding the relevant quantiles of the draws from the posterior distribution. For example, to compute 50% intervals we use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw"><a href="../reference/posterior_interval.stanreg.html">posterior_interval</a></span>(fit1, <span class="dt">prob =</span> <span class="fl">0.5</span>), <span class="dv">2</span>)</code></pre></div>
<p>For more on <code>posterior_interval</code> and interpreting the parameter estimates from a Bayesian model see Step 2 in the <a href="rstanarm.html">“How to Use the <strong>rstanarm</strong> Package”</a> vignette.</p>
<p>Using the coefficient estimates we can plot the predicted probability of <code>switch = 1</code> (as a function of <code>dist100</code>) together with the observed outcomes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Predicted probability as a function of x</span>
pr_switch &lt;-<span class="st"> </span>function(x, ests) <span class="kw">plogis</span>(ests[<span class="dv">1</span>] +<span class="st"> </span>ests[<span class="dv">2</span>] *<span class="st"> </span>x)
<span class="co"># A function to slightly jitter the binary data</span>
jitt &lt;-<span class="st"> </span>function(...) {
  <span class="kw">geom_point</span>(<span class="kw">aes_string</span>(...), <span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">height =</span> <span class="fl">0.05</span>, <span class="dt">width =</span> <span class="fl">0.1</span>), 
             <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">shape =</span> <span class="dv">21</span>, <span class="dt">stroke =</span> <span class="fl">0.2</span>)
}
<span class="kw">ggplot</span>(wells, <span class="kw">aes</span>(<span class="dt">x =</span> dist100, <span class="dt">y =</span> switch, <span class="dt">color =</span> switch)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>)) +
<span class="st">  </span><span class="kw">jitt</span>(<span class="dt">x=</span><span class="st">"dist100"</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> pr_switch, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">ests =</span> <span class="kw">coef</span>(fit1)), 
                <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">"gray35"</span>)</code></pre></div>
<p>The plot shows that under this model the predicted probability of switching is a decent bit above 50% for residents living very close to wells with safe drinking water. As expected, larger values of <code>dist100</code> are associated with lower predicted probabilities of switching. At the extreme (<span class="math inline">\(\approx 300\)</span> meters), the probability is about 25%.</p>
<p>Next, we incorporate an additional predictor into the model: the arsenic level of water in the respondent’s well. According to Gelman and Hill, “At the levels present in the Bangladesh drinking water, the health risks from arsenic are roughly proportional to exposure, and so we would expect switching to be more likely from wells with high arsenic levels” (pg. 90). We only need to change the formula, so we can use the <code>update</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit2 &lt;-<span class="st"> </span><span class="kw">update</span>(fit1, <span class="dt">formula =</span> switch ~<span class="st"> </span>dist100 +<span class="st"> </span>arsenic) </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(coef_fit2 &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">coef</span>(fit2), <span class="dv">3</span>))</code></pre></div>
<p>As expected the coefficient on <code>arsenic</code> is positive. The plot below shows distance on the x-axis and arsenic level on the y-axis with the predicted probability of well-switching mapped to the color of the background tiles (the lighter the color the higher the probability). The observed value of <code>switch</code> is indicated by the color of the points.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pr_switch2 &lt;-<span class="st"> </span>function(x, y, ests) <span class="kw">plogis</span>(ests[<span class="dv">1</span>] +<span class="st"> </span>ests[<span class="dv">2</span>] *<span class="st"> </span>x +<span class="st"> </span>ests[<span class="dv">3</span>] *<span class="st"> </span>y)
grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">dist100 =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dt">length.out =</span> <span class="dv">100</span>), 
                    <span class="dt">arsenic =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">length.out =</span> <span class="dv">100</span>))
grid$prob &lt;-<span class="st"> </span><span class="kw">with</span>(grid, <span class="kw">pr_switch2</span>(dist100, arsenic, <span class="kw">coef</span>(fit2)))
<span class="kw">ggplot</span>(grid, <span class="kw">aes</span>(<span class="dt">x =</span> dist100, <span class="dt">y =</span> arsenic)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_tile</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> prob)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> wells, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(switch)), <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">alpha =</span> <span class="fl">0.85</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_fill_gradient</span>() +
<span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="st">"switch"</span>, <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">"white"</span>, <span class="st">"black"</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">"No"</span>, <span class="st">"Yes"</span>))</code></pre></div>
<p>We can see that the black points (<code>switch=1</code>) are predominantly clustered in the upper-left region of the plot where the predicted probability of switching is highest.</p>
<p>Another way we can visualize the data and model is to follow Gelman and Hill and create separate plots for varying the arsenic level and distance. Here we’ll plot curves representing the predicted probability of switching for the minimum, maximum and quartile values of both variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">if (<span class="kw">require</span>(gridExtra)) {
  <span class="co"># Quantiles</span>
  q_ars &lt;-<span class="st"> </span><span class="kw">quantile</span>(wells$dist100, <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.25</span>))
  q_dist &lt;-<span class="st"> </span><span class="kw">quantile</span>(wells$arsenic, <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.25</span>))  
  base &lt;-<span class="st"> </span><span class="kw">ggplot</span>(wells) +<span class="st"> </span><span class="kw">xlim</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="ot">NA</span>)) +
<span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>))
  vary_arsenic &lt;-<span class="st"> </span>base +<span class="st"> </span><span class="kw">jitt</span>(<span class="dt">x=</span><span class="st">"arsenic"</span>, <span class="dt">y=</span><span class="st">"switch"</span>, <span class="dt">color=</span><span class="st">"switch"</span>)
  vary_dist &lt;-<span class="st"> </span>base +<span class="st"> </span><span class="kw">jitt</span>(<span class="dt">x=</span><span class="st">"dist100"</span>, <span class="dt">y=</span><span class="st">"switch"</span>, <span class="dt">color=</span><span class="st">"switch"</span>)
  for (i in <span class="dv">1</span>:<span class="dv">5</span>) {
    vary_dist &lt;-<span class="st"> </span>
<span class="st">      </span>vary_dist +<span class="st"> </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> pr_switch2, <span class="dt">color =</span> <span class="st">"gray35"</span>, 
                                <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">ests =</span> <span class="kw">coef</span>(fit2), <span class="dt">y =</span> q_dist[i]))
    vary_arsenic &lt;-
<span class="st">      </span>vary_arsenic +<span class="st"> </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> pr_switch2, <span class="dt">color =</span> <span class="st">"gray35"</span>, 
                                   <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">ests =</span> <span class="kw">coef</span>(fit2), <span class="dt">x =</span> q_ars[i]))
  }
  <span class="kw">grid.arrange</span>(vary_dist, vary_arsenic, <span class="dt">ncol =</span> <span class="dv">2</span>)
}</code></pre></div>
<p>We can compare our two models (with and without <code>arsenic</code>) using an approximation to Leave-One-Out (LOO) cross-validation, which is a method for estimating out of sample predictive performance and is implemented by the <code>loo</code> function in the <strong>loo</strong> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(loo1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/loo.stanreg.html">loo</a></span>(fit1))
(loo2 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/loo.stanreg.html">loo</a></span>(fit2))
<span class="kw"><a href="../reference/loo.stanreg.html">compare_models</a></span>(loo1, loo2)</code></pre></div>
<p>These results favor <code>fit2</code> over <code>fit1</code>, as the estimated difference in <code>elpd</code> (the expected log pointwise predictive density for a new dataset) is so much larger than its standard error. LOO penalizes models for adding additional predictors (this helps counter overfitting), but in this case <code>fit2</code> represents enough of an improvement over <code>fit1</code> that the penalty for including <code>arsenic</code> is negligible (as it should be if <code>arsenic</code> is an important predictor).</p>
<p>The <a href="lm.html">vignette</a> for the <code>stan_lm</code> function also has an example of using the <code>loo</code> function where the results are quite a bit different from what we see here and some important additional considerations are discussed.</p>
</div>
<div id="binomial-models" class="section level1">
<h1 class="hasAnchor">
<a href="#binomial-models" class="anchor"></a>Binomial Models</h1>
<p>Although the example in this vignette focused on a binary response variable, we can use nearly identical code if we have the sum of multiple binary variables. For example, image a hypothetical dataset similar to the well-switching data but spanning multiple villages. Each observation (each row) of this <code>data.frame</code> corresponds to an entire village: <code>switch[i]</code> is the number of ‘yes’ responses to the well-switching question for village <code>i</code>, <code>dist100[i]</code> is the average distance to the closest well with clean water for village <code>i</code>, etc. We also now have a variable <code>n</code> where <code>n[i]</code> is the number of respondents from village <code>i</code>.</p>
<p>For this data we can estimate a similar model to the one we used in the binary case by changing the formula to</p>
<p><code>cbind(switch, n - switch) ~ dist100 + arsenic</code></p>
<p>The left-hand side is now a 2-column matrix where the first column is the number of ‘yes’ responses and the second column is the number of ‘no’ responses (or more generally, the number of successes and number of failures). The same model can also be specified using the proportion of ‘yes’ responses and the total number of responses in each village. This corresponds to the formula</p>
<p><code>prop_switch ~ dist100 + arsenic</code></p>
<p>where <code>prop_switch = switch / n</code> is the proportion of ‘yes’ responses. The total number of responses is provided using the <code>weights</code> argument. In this case we would add <code>weights = n</code> to the call to <code>stan_glm</code>.</p>
<p>An example of a similar model can also be found in <strong>Step 1</strong> of the <a href="rstanarm.html">“How to Use the <strong>rstanarm</strong> Package”</a> vignette.</p>
</div>
<div id="going-further" class="section level1">
<h1 class="hasAnchor">
<a href="#going-further" class="anchor"></a>Going Further</h1>
<p>In the hypothetical scenario above, if we also have access to the observations for each individual in all of the villages (not just the aggregate data), then a natural extension would be to consider a multilevel model that takes advantage of the inherent multilevel structure of the data (individuals nested within villages). The <a href="glmer.html">vignette</a> for the <code>stan_glmer</code> function discusses these models.</p>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<p>Gelman, A. and Hill, J. (2007). <em>Data Analysis Using Regression and Multilevel/Hierarchical Models.</em> Cambridge University Press, Cambridge, UK.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#likelihood">Likelihood</a></li>
      <li><a href="#priors">Priors</a></li>
      <li><a href="#posterior">Posterior</a></li>
      <li><a href="#logistic-regression-example">Logistic Regression Example</a></li>
      <li><a href="#binomial-models">Binomial Models</a></li>
      <li><a href="#going-further">Going Further</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Jonah Gabry, Ben Goodrich.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
